{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 爬蟲的基本觀念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 學習 Python 爬蟲幾個重要的學習歷程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- beautifulsoup 與 lxml 模塊的差異與選擇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 使用 requests + lxml 建立我們的第一支爬蟲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 其他補充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :\n",
    "\n",
    "- <font color=\"red\">爬蟲的基本觀念</font>\n",
    "\n",
    "- 學習 Python 爬蟲幾個重要的學習歷程\n",
    "\n",
    "- beautifulsoup 與 lxml 模塊的差異與選擇\n",
    "\n",
    "- 使用 requests + lxml 建立我們的第一支爬蟲\n",
    "\n",
    "- 提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲\n",
    "\n",
    "- 其他補充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 爬蟲的基本觀念 :\n",
    "\n",
    "1. 在瀏覽器輸入網址後會往遠端Server送出請求(requests)\n",
    "\n",
    "   (<font color=\"red\">在爬蟲中就是以 GET 或是 POST 的方式發出請求</font>)\n",
    "\n",
    "2. 遠端伺服器收到請求後會做出回應(Response)，\n",
    "\n",
    "    會將相關的 html / css 原始碼回傳回來\n",
    "\n",
    "   ，並且透過瀏覽器轉譯後就會變成我們看到的精細頁面\n",
    "   \n",
    "   (<font color=\"red\">但網路爬蟲看到的是一連串的原始碼，必須經過一定的剖析</font>\n",
    "   \n",
    "   <font color=\"red\">與整理才能把想要的資訊抽取出來</font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 爬蟲的基本觀念 :\n",
    "\n",
    "<img src=\"./reveal.js/img/webscraping_rmb.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!doctype html>\n",
      "<html xmlns:ng=\"http://angularjs.org\" ng-app=\"kuobrothersApp\" id=\"ng-app\" lang=\"zh-Hant-TW\">\n",
      "\t<head>\n",
      "\t\t<meta charset=\"UTF-8\">\n",
      "\t\t<meta name=\"gby\" content=\"\n",
      "                                 |~~~~~~~|\n",
      "                                 |       |\n",
      "                                 |       |\n",
      "                                 |       |\n",
      "                                 |       |\n",
      "                                 |       |\n",
      "      |~.\\\\\\_\\~~~~~~~~~~~~~~xx~~~         ~~~~~~~~~~~~~~~~~~~~~/_//;~|\n",
      "      |  \\  o \\_         ,XXXXX),                         _..-~ o /  |\n",
      "      |    ~~\\  ~-.     XXXXX`)))),                 _.--~~   .-~~~   |\n",
      "       ~~~~~~~`\\   ~\\~~~XXX' _/ ';))     |~~~~~~..-~     _.-~ ~~~~~~~ \n",
      "                `\\   ~~--`_\\~\\, ;;;\\)__.---.~~~      _.-~\n",
      "                  ~-.       `:;;/;; \\          _..-~~\n",
      "                     ~-._      `''        /-~-~ \n",
      "                         `\\              /  /\n",
      "                           |         ,   | |\n",
      "                            |  '   \n"
     ]
    }
   ],
   "source": [
    "### Python 爬蟲中 GET 方法的使用 :\n",
    "\n",
    "import requests\n",
    "\n",
    "res = requests.get(\"https://www.buy123.com.tw/\")\n",
    "\n",
    "print res.text[1:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :\n",
    "\n",
    "- 爬蟲的基本觀念\n",
    "\n",
    "- <font color=\"red\">學習 Python 爬蟲幾個重要的學習歷程</font>\n",
    "\n",
    "- beautifulsoup 與 lxml 模塊的差異與選擇\n",
    "\n",
    "- 使用 requests + lxml 建立我們的第一支爬蟲\n",
    "\n",
    "- 提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲\n",
    "\n",
    "- 其他補充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 學習 Python 爬蟲幾個重要的學習歷程 :\n",
    "\n",
    "1. Python 基礎知識\n",
    "\n",
    "2. 使用 Python 連接資料庫進行儲存 \n",
    "\n",
    "3. Python 正則表達式\n",
    "\n",
    "4. Python 爬蟲框架 Scrapy\n",
    "\n",
    "5. Python 爬蟲更高級的功能(Selenium, proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :\n",
    "\n",
    "- 爬蟲的基本觀念\n",
    "\n",
    "- 學習 Python 爬蟲幾個重要的學習歷程\n",
    "\n",
    "- <font color=\"red\">beautifulsoup 與 lxml 模塊的差異與選擇</font>\n",
    "\n",
    "- 使用 requests + lxml 建立我們的第一支爬蟲\n",
    "\n",
    "- 提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲\n",
    "\n",
    "- 其他補充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BeautifulSoup 與 lxml 模塊的差異與選擇 :\n",
    "\n",
    "- <font color=\"red\">性能 lxml >>> BeautifulSoup !</font>\n",
    "\n",
    "- <font color=\"red\">易用 BeautifulSoup >>> lxml !</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 性能 lxml >>> BeautifulSoup :\n",
    "\n",
    "BeautifulSoup 和 lxml 的解析原理不同 ， \n",
    "\n",
    "BeautifulSoup 是使用DOM Tree方式解析的 ， \n",
    "\n",
    "會載入整個文檔，並解析整個 DOM Tree，因此時間和內存開銷都會大很多。\n",
    "\n",
    "另外 lxml 是用c寫的，而 BeautifulSoup 是用 python 寫的，因此性能方面自然會差很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 易用性 BeautifulSoup >> lxml :\n",
    "\n",
    "lxml 的 XPath 要求一定清楚文檔結構，可以使用絕對路徑或相對路徑查找，\n",
    "\n",
    "而 beautifulsoup 不必清楚文檔結構，可以直接找某些標籤，簡單粗暴。\n",
    "\n",
    "<font color=\"red\">兩者 code 差別如下 :</font>\n",
    "\n",
    "title = soup.select('.content div.title h3')\n",
    "\n",
    "title = tree.xpath(\"//*[@class='content']/div[@class='content']/h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### lxml 在 python 爬蟲中的操作 :\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "res = requests.get(\"https://www.buy123.com.tw/\")\n",
    "# 從 print res.text 前幾行的結果可以看出是採 UTF-8 進行編碼\n",
    "res.encoding = 'utf8'\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# 使用 lxml 的 HTMLParser 進行解析\n",
    "root = etree.fromstring(res.content, etree.HTMLParser()) \n",
    "# 使用 xpath 搜索並取得欲收集的資料\n",
    "root.xpath('//*[@id=\"container\"]/div[4]/section[2]//a/@href')\n",
    "\n",
    "#-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### 幾種常用的 xpath 寫法 :\n",
    "\n",
    "# 1. 抓出所有連結的網址 (a下href) : \n",
    "xpath = \"...//a/@href\"\n",
    "\n",
    "# 2. 抓出路徑下所有文字 :\n",
    "xpath = \".../text()\"\n",
    "\n",
    "# 3. 抓出 div 下 id=\"xxx\" 的物件 :\n",
    "xpath = \"//div[@id='xxx']\"\n",
    "\n",
    "# 4. 抓出所有 td 下 class 有 \"string\" 字串的物件 :\n",
    "xpath = \"//td[contains(@class, 'string')]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :\n",
    "\n",
    "- 爬蟲的基本觀念\n",
    "\n",
    "- 學習 Python 爬蟲幾個重要的學習歷程\n",
    "\n",
    "- beautifulsoup 與 lxml 模塊的差異與選擇\n",
    "\n",
    "- <font color=\"red\">使用 requests + lxml 建立我們的第一支爬蟲</font>\n",
    "\n",
    "- 提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲\n",
    "\n",
    "- 其他補充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 使用 requests + lxml 建立我們的第一支爬蟲 :\n",
    "\n",
    "- 爬取目標 : 生活市集\n",
    "\n",
    "- 使用工具 : requests + lxml\n",
    "\n",
    "- 常用 python 語法 : try except"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 爬取目標 : 生活市集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "input website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 先載入必備的 modules :\n",
    "from lxml import etree\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 使用 requests 的 GET，並使用 lxml 的 etree 去進行解析(Parse)\n",
    "res = requests.get(\"https://www.buy123.com.tw/\")\n",
    "res.encoding = 'utf8'\n",
    "root = etree.fromstring(res.content, etree.HTMLParser()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 取得所有商品 urls\n",
    "list1 = []\n",
    "for each in root.xpath('//*[@id=\"container\"]/div[4]/section[2]//a/@href'):\n",
    "    list1.append(\"https://www.buy123.com.tw/\" + each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 建立一個空的 dict 以存放不重覆資料\n",
    "total_item = {}\n",
    "for each_url in list1:\n",
    "    res = requests.get(each_url)\n",
    "    res.encoding = 'utf8'\n",
    "    root = etree.fromstring(res.content, etree.HTMLParser())\n",
    "    # 使用 try except 以處理少數不規則網頁 , 若規則不同先以 a 取代以避免迴圈中斷\n",
    "    try:  \n",
    "        # 進入個網頁分別抓取 \"產品名稱\"、\"價格\"、\"累計購買人數\"\n",
    "        tmp_item = root.xpath('//*[@id=\"deal_detail_info\"]/div[1]/div/h1/text()')[0]\n",
    "        tmp_price = root.xpath('//*[@id=\"price\"]/text()')[1]\n",
    "        tmp_sold = root.xpath('//*[@id=\"deal_price_detail\"]/div[7]/text()')[0]\n",
    "    except:\n",
    "        tmp_item = 'na'\n",
    "        tmp_price = 'na'\n",
    "        tmp_sold = 'na'\n",
    "    # 最後將收集到的資料放回 dict 中\n",
    "    total_item[tmp_item] = [tmp_price,tmp_sold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>舒爽透氣條紋明根平口褲</td>\n",
       "      <td>[59, 2502]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>魔力回彈人魚線健腹輪</td>\n",
       "      <td>[599, 300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>兒童超防風帽帶書包雨衣</td>\n",
       "      <td>[226, 841]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>超強光雙燈太陽能露營燈</td>\n",
       "      <td>[189, 984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>四合一多功能手機運動包</td>\n",
       "      <td>[159, 291]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1\n",
       "3363  舒爽透氣條紋明根平口褲  [59, 2502]\n",
       "3364   魔力回彈人魚線健腹輪  [599, 300]\n",
       "3365  兒童超防風帽帶書包雨衣  [226, 841]\n",
       "3366  超強光雙燈太陽能露營燈  [189, 984]\n",
       "3367  四合一多功能手機運動包  [159, 291]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看一下抓取到的資料樣貌，後 5 筆\n",
    "pd.DataFrame(total_item.items()).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :\n",
    "\n",
    "- 爬蟲的基本觀念\n",
    "\n",
    "- 學習 Python 爬蟲幾個重要的學習歷程\n",
    "\n",
    "- beautifulsoup 與 lxml 模塊的差異與選擇\n",
    "\n",
    "- 使用 requests + lxml 建立我們的第一支爬蟲\n",
    "\n",
    "- <font color=\"red\">提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲</font>\n",
    "\n",
    "- 其他補充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 基本觀念 ： 多進程 v.s 多線程 ： 任務 & 子任務\n",
    "\n",
    "1. 何謂多任務 ? 操作系統可以一次運行多個任務\n",
    "\n",
    "2. (單核)系統如何運行多任務 ？ 快速輪流切換執行\n",
    "\n",
    "3. 一個任務就是一個進程（Process）\n",
    "\n",
    "4. 一個進程(Process)有 >= 1 個子任務(線程, Thread)(一個進程至少有一個線程)\n",
    "\n",
    "5. 系統如何運行多任務 ？ 快速輪流切換執行(同Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1. 多任務 > 比如說一般再使用電腦時可以開網頁 + 聽音樂 + 開遊戲 + 發個email\n",
    "\n",
    "2. 系統如何運行 ： \n",
    "    2.1 操作系統輪流讓各個任務交替執行，任務1執行0.01秒，切換到任務2，任務2執行0.01秒，再切換到任務3，執行0.01秒……這樣反覆執行下去。 \n",
    "    2.2 就算每個任務都是交替執行的， 但是， 由於CPU的執行速度實在是太快了，我們感覺就像所有任務都在同時執行一樣。\n",
    "    2.3 真正的 \" 平行執行多任務\" 只能在多核CPU上實現，但是，由於任務數量往往遠遠多於CPU的核心數量，所以，操作系統也會自動把很多任務輪流調度到每個核心上執行。\n",
    "    \n",
    "3. 比如打開一個瀏覽器就是啟動一個進程，打開一個記事本也是啟動了一個進程，打開一個Word也是。\n",
    "\n",
    "4. 一個進程(Process)有 >= 1 個子任務(線程, Thread):\n",
    "    4.1 有些進程還不止同時做一件事，比如Word，它可以同時進行打字、拼寫檢查等事情。\n",
    "    4.2 在一個進程內部， 要同時作很多事， 即同時有多個“子任務”， 而這些“子任務”就稱為線程（Thread）。\n",
    "    \n",
    "5. 也是由操作系統在多個線程之間快速切換，讓每個線程都短暫地交替運行，看起來就像同時執行一樣。當然，真正地同時執行多線程需要多核CPU才可能實現。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 基本觀念 ： 多進程 v.s 多線程 ： Python 簡介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 在沒有特別調整況下， 編寫出來的 Python code 都是單線程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 如何優化我們的 Python code 效率 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 總結如下 ： \n",
    "    - <font color=\"red\">Multi Process 1 Thread (多進程)</font>\n",
    "    - <font color=\"red\">1 Process Multi Thread (多線程)</font>\n",
    "    - ~~Multi Process Multi Thread~~  <font color=\"gray\">這種模型很複雜，實際很少使用</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### 多進程 (Multi Process 1 Thread) Python 實現 ：\n",
    "from lxml import etree\n",
    "import requests\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "res = requests.get(\"https://www.buy123.com.tw/\")\n",
    "res.encoding = 'utf8'\n",
    "root = etree.fromstring(res.content, etree.HTMLParser()) \n",
    "urllist = []\n",
    "for each in root.xpath('//*[@id=\"container\"]/div[4]/section[2]//a/@href'):\n",
    "    urllist.append(\"https://www.buy123.com.tw/\" + each)\n",
    "\n",
    "def get_data(urllist):\n",
    "    total_item = {}\n",
    "    res = requests.get(urllist)\n",
    "    res.encoding = 'utf8'\n",
    "    root = etree.fromstring(res.content, etree.HTMLParser())\n",
    "    try:\n",
    "        tmp_item = root.xpath('//*[@id=\"deal_detail_info\"]/div[1]/div/h1/text()')[0]\n",
    "        tmp_price = root.xpath('//*[@id=\"price\"]/text()')[1]\n",
    "        tmp_sold = root.xpath('//*[@id=\"deal_price_detail\"]/div[7]/text()')[0]\n",
    "    except:\n",
    "        tmp_item = 'na'\n",
    "        tmp_price = 'na'\n",
    "        tmp_sold = 'na'\n",
    "    total_item[tmp_item] = [tmp_price,tmp_sold]\n",
    "    return total_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    scraped_data = []\n",
    "    \n",
    "    starttime = datetime.now()    \n",
    "    \n",
    "    p = Pool(7)\n",
    "    scraped_data.append(p.map(get_data,urllist))\n",
    "    df1 = pd.DataFrame(scraped_data)\n",
    "\n",
    "    endtime = datetime.now()\n",
    "    runtime = endtime - starttime\n",
    "    print \"Total run time : \"+runtime\n",
    "    # len(scraped_data[0]) = 3387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Webscraping 分享綱要 :\n",
    "\n",
    "- 爬蟲的基本觀念\n",
    "\n",
    "- 學習 Python 爬蟲幾個重要的學習歷程\n",
    "\n",
    "- beautifulsoup 與 lxml 模塊的差異與選擇\n",
    "\n",
    "- 使用 requests + lxml 建立我們的第一支爬蟲\n",
    "\n",
    "- 提升爬蟲效率 ： 利用 multiprocessing 模快實現多進程爬蟲\n",
    "\n",
    "- <font color=\"red\">其他補充</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 心法 & 觀念 :\n",
    "\n",
    "#### Ryan Mitchell : \n",
    "\n",
    "#####  <font color=\"red\">網路爬蟲的第一原則是 : 所有訊息都可以造假</font>\n",
    "\n",
    "#### Wush大大 :\n",
    "\n",
    "#####  <font color=\"red\">盡量匿蹤原則</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 心法 & 觀念 :\n",
    "\n",
    "1. 儘量第一次發Query就做對，不要有太多異常的Query。\n",
    "2. 尊重和珍惜網路流量資源。不要吃光對方的流量，也不要干擾到對方的正常服務。\n",
    "3. 可以用Proxy服務作跳板，如TOR，但是請小心謹慎的使用，珍惜公共資源。\n",
    "4. 儘量在使用者多得時候混在使用者中，避免在離峰時間出沒以免被注意。\n",
    "5. 發Query的行為儘量偽裝成正常使用者，如：\n",
    "    5.1 每抓取一筆資料後，隨機等待一段時間後再行動(使用 sleep + rand 函數)。\n",
    "    5.2 每天自不同的時間開始行動，亦即系統排程不要太有規律。\n",
    "    5.3 限制每天抓取的總流量，這樣才能細水長流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 其他資源補充 :\n",
    "\n",
    "1. W3C xpath syntax : http://www.w3schools.com/xsl/xpath_syntax.asp\n",
    "\n",
    "2. 大數學堂 : http://www.largitdata.com/\n",
    "\n",
    "3. DataCamp : https://www.datacamp.com/\n",
    "\n",
    "4. Python Regular Expression : https://docs.python.org/2/howto/regex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 資料引用說明 :\n",
    "\n",
    "大數學堂相關影片與文章, wush相關文章, dsp相關文章, 北京工業彭泉鑫文章"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
